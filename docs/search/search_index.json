{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fundamentaci\u00f3n El presente proyecto de tesina se enmarca en el proyecto \u201cAn\u00e1lisis Prospectivo Inteligente Del Impacto Social, Econ\u00f3mico Y Productivo Del Covid-19 En La Provincia De Chubut\u201d, el cual fue presentado y aprobado en la convocatoria PROGRAMA DE ARTICULACI\u00d3N Y FORTALECIMIENTO FEDERAL DE LAS CAPACIDADES EN CIENCIA Y TECNOLOG\u00cdA COVID-19. En este marco, el proyecto de tesina aborda una tem\u00e1tica particular y espec\u00edfica dentro del mencionado proyecto, la cual se fundamenta a continuaci\u00f3n. La vida de las personas y el curso de las actividades y servicios no esenciales han sufrido cambios dr\u00e1sticos a partir del Decreto DECNU-2020-297-APN-PTE y sus normativas anexas de nivel provincial y/o municipal en Chubut. Las medidas de Aislamiento Social, Preventivo y Obligatorio (ASPO) han generado y continuar\u00e1n generando desencadenantes de impacto social y econ\u00f3mico que necesitan ser identificados y monitoreados para proveer informaci\u00f3n a los formuladores de pol\u00edticas p\u00fablicas. Si bien existen herramientas y emprendimientos a nivel nacional, la provincia de Chubut necesita fortalecer el conjunto de instrumentos que sean capaces de determinar el c\u00famulo de variables involucradas, medirlas y transformarlas en indicadores cuantitativos y cualitativos que aporten a un tablero de an\u00e1lisis de impacto provincial. Este proyecto propone la construcci\u00f3n de conocimiento a partir de diferentes estrategias y herramientas de relevamiento de informaci\u00f3n y datos, que posibiliten un an\u00e1lisis, procesamiento y ponderaci\u00f3n de la situaci\u00f3n actual, as\u00ed como una predicci\u00f3n futura del estado de un territorio en t\u00e9rminos sociales, econ\u00f3micos y productivos, en virtud de una circunstancia sanitaria como la generada por el COVID-19. En particular se considerar\u00e1 la regi\u00f3n delimitada por los l\u00edmites geogr\u00e1ficos de la provincia del Chubut, ajustando la escala territorial a nivel de ciudades, pueblos y comunas rurales. La construcci\u00f3n de conocimiento, se har\u00e1 a partir de la extracci\u00f3n, procesamiento y an\u00e1lisis autom\u00e1tico de informaci\u00f3n period\u00edstica publicada en la prensa provincial, la cual luego ser\u00e1 presentada de manera acorde, a fin de poder evaluarse de manera indirecta la evoluci\u00f3n de diferentes t\u00f3picos, que impactan en la sociedad. Los resultados ser\u00e1n obtenidos mediante la aplicaci\u00f3n de t\u00e9cnicas de Procesamiento de Lenguaje Natural (NLP) y extracci\u00f3n de datos de sitios web (web scraping), para luego ser presentados en una aplicaci\u00f3n web que permitir\u00e1 su an\u00e1lisis y/o difusi\u00f3n. Antecedentes El alumno Emanuel Balcazar ha trabajado, bajo la direcci\u00f3n del Dr. Ing. Ordinez, en un sistema de recopilaci\u00f3n, procesamiento y visualizaci\u00f3n de profesionales de Chubut, denominado \u201cGeoPerfil Profesional\u201d, el cual fue presentado, aprobado y financiado por la convocatoria Universidades Agregado Valor 2017 de la Secretar\u00eda de Pol\u00edticas Universitarias. En dicho proyecto, se utilizaron m\u00e9todos de extracci\u00f3n de datos de diversas fuentes (scrapping) y procesamiento del lenguaje natural (NLP) para llevar a cabo el proyecto, adem\u00e1s de implementar una aplicaci\u00f3n web para visualizar los resultados. Objetivos Objetivo General Construcci\u00f3n de conocimiento acerca de la situaci\u00f3n de la provincia del Chubut en el marco de la pandemia por COVID-19 mediante la extracci\u00f3n y el procesamiento autom\u00e1tico de informaci\u00f3n de fuentes period\u00edsticas sobre t\u00f3picos afectados por el contexto sanitario: econom\u00eda, educaci\u00f3n, movilidad, producci\u00f3n, vulnerabilidad. Objetivos Especificos Investigar y recopilar la informaci\u00f3n disponible, as\u00ed como herramientas de trabajo y proyectos similares. Identificar y caracterizar fuentes period\u00edsticas de la provincia del Chubut en t\u00e9rminos de extracci\u00f3n autom\u00e1tica de sus notas. Implementar un vocabulario que sintetice, mediante un conjunto de palabras, un t\u00f3pico particular. Generar una herramienta de extracci\u00f3n autom\u00e1tica de informaci\u00f3n de fuentes period\u00edsticas. Analizar la informaci\u00f3n extra\u00edda mediante t\u00e9cnicas de Procesamiento de Lenguaje Natural que permitan generar nueva informaci\u00f3n y/o representarla de manera accesible. Mostrar los resultados utilizando diversas herramientas inform\u00e1ticas (como gr\u00e1ficos, mapas, etc) que sinteticen los resultados obtenidos. Desarrollo Propuesto Las fuentes de informaci\u00f3n abiertas de donde se obtendr\u00e1n los datos son p\u00e1ginas de diarios y noticias online en donde se mencionan ciertos t\u00f3picos (econom\u00eda, salud, sentimientos, etc) relacionados con la reciente pandemia. En el aspecto tecnol\u00f3gico se evaluaran las herramientas a utilizar, siendo \u00e9stas frameworks que permitan implementar los procesos que se necesiten para generar los resultados. Tambi\u00e9n se tendr\u00e1n en cuenta las herramientas de representaci\u00f3n de datos como mapas, gr\u00e1ficos estad\u00edsticos u otros que sean de inter\u00e9s para sintetizar la informaci\u00f3n resultante y que permitan en todos los casos analizar la evoluci\u00f3n temporal de los contenidos extra\u00eddos. Se utilizaran t\u00e9cnicas de procesamiento de lenguaje natural para interpretar y generar informaci\u00f3n a partir de los datos que se obtengan durante la fase de extracci\u00f3n de art\u00edculos, lo cual se realizar\u00e1 de manera autom\u00e1tica de tal forma que continuamente se disponga de nueva informaci\u00f3n. Se podr\u00e1n hacer uso de librer\u00edas gr\u00e1ficas para representar la informaci\u00f3n con mapas interactivos, gr\u00e1ficos estad\u00edsticos y todo lo que se requiera para lograr un resultado accesible y legible para el p\u00fablico. Como complemento se analizar\u00e1 la factibilidad de hacer uso del \u201can\u00e1lisis de sentimientos\u201d sobre la opini\u00f3n de la prensa respecto a c\u00f3mo ha afectado la pandemia y el permanecer durante tanto tiempo en cuarentena permitiendo conocer un poco m\u00e1s el impacto social en Chubut. Adem\u00e1s se podr\u00eda construir, a partir de las emociones reflejadas en las notas, un gr\u00e1fico que muestre por zona en que lugares a impactado de manera m\u00e1s negativa la pandemia. Estos aspectos ser\u00e1n primeramente analizados en cuanto a su factibilidad y potencial de representatividad estad\u00edstica. Resultados Esperados Se esperan alcanzar los siguientes hitos, como parte del proceso pedag\u00f3gico consistente en un desarrollo mayormente aut\u00f3nomo y de complejidad acorde un trabajo profesional. Amplitud, criterio y autonom\u00eda en la b\u00fasqueda de informaci\u00f3n, selecci\u00f3n de tecnolog\u00eda y dimensionamiento de la complejidad para un proyecto. Conocimiento de las problem\u00e1ticas y alternativas de soluci\u00f3n para una situaci\u00f3n difusamente planteada (realidad). Sistematizaci\u00f3n y organizaci\u00f3n de tareas, m\u00e9todos y materiales para alcanzar objetivos. Construcci\u00f3n de un sistema de extracci\u00f3n, an\u00e1lisis y visualizaci\u00f3n de informaci\u00f3n period\u00edstica del impacto en distintas dimensiones de la pandemia por COVID-19 en la provincia del Chubut, el cual se encuentre desplegado en un ambiente de producci\u00f3n. Cronograma de Actividades # Actividad Duraci\u00f3n Aprox. 1 Inmersi\u00f3n y estado del arte en extracci\u00f3n autom\u00e1tica y procesamiento de lenguaje natural de art\u00edculos period\u00edsticos 1 semana 2 Arquitectura la soluci\u00f3n y selecci\u00f3n de fuentes 1 semana 3 Estudio de vocabulario 2 semanas 4 Experimentaci\u00f3n con tecnolog\u00edas 2 semanas 5 Dise\u00f1o, implementaci\u00f3n y prueba de extractor 3 semanas 6 Dise\u00f1o, implementaci\u00f3n y prueba de analizador 3 semanas 7 Integraci\u00f3n, pruebas y correcciones de visualizador 3 semanas Total 15 semanas","title":"Principal"},{"location":"#fundamentacion","text":"El presente proyecto de tesina se enmarca en el proyecto \u201cAn\u00e1lisis Prospectivo Inteligente Del Impacto Social, Econ\u00f3mico Y Productivo Del Covid-19 En La Provincia De Chubut\u201d, el cual fue presentado y aprobado en la convocatoria PROGRAMA DE ARTICULACI\u00d3N Y FORTALECIMIENTO FEDERAL DE LAS CAPACIDADES EN CIENCIA Y TECNOLOG\u00cdA COVID-19. En este marco, el proyecto de tesina aborda una tem\u00e1tica particular y espec\u00edfica dentro del mencionado proyecto, la cual se fundamenta a continuaci\u00f3n. La vida de las personas y el curso de las actividades y servicios no esenciales han sufrido cambios dr\u00e1sticos a partir del Decreto DECNU-2020-297-APN-PTE y sus normativas anexas de nivel provincial y/o municipal en Chubut. Las medidas de Aislamiento Social, Preventivo y Obligatorio (ASPO) han generado y continuar\u00e1n generando desencadenantes de impacto social y econ\u00f3mico que necesitan ser identificados y monitoreados para proveer informaci\u00f3n a los formuladores de pol\u00edticas p\u00fablicas. Si bien existen herramientas y emprendimientos a nivel nacional, la provincia de Chubut necesita fortalecer el conjunto de instrumentos que sean capaces de determinar el c\u00famulo de variables involucradas, medirlas y transformarlas en indicadores cuantitativos y cualitativos que aporten a un tablero de an\u00e1lisis de impacto provincial. Este proyecto propone la construcci\u00f3n de conocimiento a partir de diferentes estrategias y herramientas de relevamiento de informaci\u00f3n y datos, que posibiliten un an\u00e1lisis, procesamiento y ponderaci\u00f3n de la situaci\u00f3n actual, as\u00ed como una predicci\u00f3n futura del estado de un territorio en t\u00e9rminos sociales, econ\u00f3micos y productivos, en virtud de una circunstancia sanitaria como la generada por el COVID-19. En particular se considerar\u00e1 la regi\u00f3n delimitada por los l\u00edmites geogr\u00e1ficos de la provincia del Chubut, ajustando la escala territorial a nivel de ciudades, pueblos y comunas rurales. La construcci\u00f3n de conocimiento, se har\u00e1 a partir de la extracci\u00f3n, procesamiento y an\u00e1lisis autom\u00e1tico de informaci\u00f3n period\u00edstica publicada en la prensa provincial, la cual luego ser\u00e1 presentada de manera acorde, a fin de poder evaluarse de manera indirecta la evoluci\u00f3n de diferentes t\u00f3picos, que impactan en la sociedad. Los resultados ser\u00e1n obtenidos mediante la aplicaci\u00f3n de t\u00e9cnicas de Procesamiento de Lenguaje Natural (NLP) y extracci\u00f3n de datos de sitios web (web scraping), para luego ser presentados en una aplicaci\u00f3n web que permitir\u00e1 su an\u00e1lisis y/o difusi\u00f3n.","title":"Fundamentaci\u00f3n"},{"location":"#antecedentes","text":"El alumno Emanuel Balcazar ha trabajado, bajo la direcci\u00f3n del Dr. Ing. Ordinez, en un sistema de recopilaci\u00f3n, procesamiento y visualizaci\u00f3n de profesionales de Chubut, denominado \u201cGeoPerfil Profesional\u201d, el cual fue presentado, aprobado y financiado por la convocatoria Universidades Agregado Valor 2017 de la Secretar\u00eda de Pol\u00edticas Universitarias. En dicho proyecto, se utilizaron m\u00e9todos de extracci\u00f3n de datos de diversas fuentes (scrapping) y procesamiento del lenguaje natural (NLP) para llevar a cabo el proyecto, adem\u00e1s de implementar una aplicaci\u00f3n web para visualizar los resultados.","title":"Antecedentes"},{"location":"#objetivos","text":"","title":"Objetivos"},{"location":"#objetivo-general","text":"Construcci\u00f3n de conocimiento acerca de la situaci\u00f3n de la provincia del Chubut en el marco de la pandemia por COVID-19 mediante la extracci\u00f3n y el procesamiento autom\u00e1tico de informaci\u00f3n de fuentes period\u00edsticas sobre t\u00f3picos afectados por el contexto sanitario: econom\u00eda, educaci\u00f3n, movilidad, producci\u00f3n, vulnerabilidad.","title":"Objetivo General"},{"location":"#objetivos-especificos","text":"Investigar y recopilar la informaci\u00f3n disponible, as\u00ed como herramientas de trabajo y proyectos similares. Identificar y caracterizar fuentes period\u00edsticas de la provincia del Chubut en t\u00e9rminos de extracci\u00f3n autom\u00e1tica de sus notas. Implementar un vocabulario que sintetice, mediante un conjunto de palabras, un t\u00f3pico particular. Generar una herramienta de extracci\u00f3n autom\u00e1tica de informaci\u00f3n de fuentes period\u00edsticas. Analizar la informaci\u00f3n extra\u00edda mediante t\u00e9cnicas de Procesamiento de Lenguaje Natural que permitan generar nueva informaci\u00f3n y/o representarla de manera accesible. Mostrar los resultados utilizando diversas herramientas inform\u00e1ticas (como gr\u00e1ficos, mapas, etc) que sinteticen los resultados obtenidos.","title":"Objetivos Especificos"},{"location":"#desarrollo-propuesto","text":"Las fuentes de informaci\u00f3n abiertas de donde se obtendr\u00e1n los datos son p\u00e1ginas de diarios y noticias online en donde se mencionan ciertos t\u00f3picos (econom\u00eda, salud, sentimientos, etc) relacionados con la reciente pandemia. En el aspecto tecnol\u00f3gico se evaluaran las herramientas a utilizar, siendo \u00e9stas frameworks que permitan implementar los procesos que se necesiten para generar los resultados. Tambi\u00e9n se tendr\u00e1n en cuenta las herramientas de representaci\u00f3n de datos como mapas, gr\u00e1ficos estad\u00edsticos u otros que sean de inter\u00e9s para sintetizar la informaci\u00f3n resultante y que permitan en todos los casos analizar la evoluci\u00f3n temporal de los contenidos extra\u00eddos. Se utilizaran t\u00e9cnicas de procesamiento de lenguaje natural para interpretar y generar informaci\u00f3n a partir de los datos que se obtengan durante la fase de extracci\u00f3n de art\u00edculos, lo cual se realizar\u00e1 de manera autom\u00e1tica de tal forma que continuamente se disponga de nueva informaci\u00f3n. Se podr\u00e1n hacer uso de librer\u00edas gr\u00e1ficas para representar la informaci\u00f3n con mapas interactivos, gr\u00e1ficos estad\u00edsticos y todo lo que se requiera para lograr un resultado accesible y legible para el p\u00fablico. Como complemento se analizar\u00e1 la factibilidad de hacer uso del \u201can\u00e1lisis de sentimientos\u201d sobre la opini\u00f3n de la prensa respecto a c\u00f3mo ha afectado la pandemia y el permanecer durante tanto tiempo en cuarentena permitiendo conocer un poco m\u00e1s el impacto social en Chubut. Adem\u00e1s se podr\u00eda construir, a partir de las emociones reflejadas en las notas, un gr\u00e1fico que muestre por zona en que lugares a impactado de manera m\u00e1s negativa la pandemia. Estos aspectos ser\u00e1n primeramente analizados en cuanto a su factibilidad y potencial de representatividad estad\u00edstica.","title":"Desarrollo Propuesto"},{"location":"#resultados-esperados","text":"Se esperan alcanzar los siguientes hitos, como parte del proceso pedag\u00f3gico consistente en un desarrollo mayormente aut\u00f3nomo y de complejidad acorde un trabajo profesional. Amplitud, criterio y autonom\u00eda en la b\u00fasqueda de informaci\u00f3n, selecci\u00f3n de tecnolog\u00eda y dimensionamiento de la complejidad para un proyecto. Conocimiento de las problem\u00e1ticas y alternativas de soluci\u00f3n para una situaci\u00f3n difusamente planteada (realidad). Sistematizaci\u00f3n y organizaci\u00f3n de tareas, m\u00e9todos y materiales para alcanzar objetivos. Construcci\u00f3n de un sistema de extracci\u00f3n, an\u00e1lisis y visualizaci\u00f3n de informaci\u00f3n period\u00edstica del impacto en distintas dimensiones de la pandemia por COVID-19 en la provincia del Chubut, el cual se encuentre desplegado en un ambiente de producci\u00f3n.","title":"Resultados Esperados"},{"location":"#cronograma-de-actividades","text":"# Actividad Duraci\u00f3n Aprox. 1 Inmersi\u00f3n y estado del arte en extracci\u00f3n autom\u00e1tica y procesamiento de lenguaje natural de art\u00edculos period\u00edsticos 1 semana 2 Arquitectura la soluci\u00f3n y selecci\u00f3n de fuentes 1 semana 3 Estudio de vocabulario 2 semanas 4 Experimentaci\u00f3n con tecnolog\u00edas 2 semanas 5 Dise\u00f1o, implementaci\u00f3n y prueba de extractor 3 semanas 6 Dise\u00f1o, implementaci\u00f3n y prueba de analizador 3 semanas 7 Integraci\u00f3n, pruebas y correcciones de visualizador 3 semanas Total 15 semanas","title":"Cronograma de Actividades"},{"location":"aplicacion-servidor/","text":"Aplicaci\u00f3n servidor A continuaci\u00f3n se le brindar\u00e1 informaci\u00f3n basica sobre como es el funcionamiento del framework y que debe hacer si desea agregar nuevas funcionalidades, esto es solo un vistazo por lo que se recomienda fuertemente que visite la documentaci\u00f3n de la pagina oficial si desea tener mas detalles. https://adonisjs.com/docs/4.1/about Estructura del servidor P\u00e1gina de referencia La estructura generada por AdonisJS consiste en los siguientes directorios proyecto \u251c\u2500\u2500 app \u251c\u2500\u2500 config \u251c\u2500\u2500 database \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 providers \u251c\u2500\u2500 start \u251c\u2500\u2500 test \u251c\u2500\u2500 .env \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 package.json \u251c\u2500\u2500 ace \u251c\u2500\u2500 server.js \u251c\u2500\u2500 vowfile.js \u251c\u2500\u2500 README.md A continuaci\u00f3n describiremos los directorios y archivos m\u00e1s importantes para que nos ayuden a entender m\u00e1s el funcionamiento del framework. app : Contiene los archivos necesarios para el funcionamiento basico de la aplicaci\u00f3n. A su vez contiene las carpetas: Controllers como presenter, Middleware en donde se encuentran los middlewares propios y Models en donde se encuentran los modelos asociados a las tablas de la base de datos. config : Como el nombre indica, contiene todos los archivos de configuracion de la aplicaci\u00f3n. database : Contiene los migration y seeds para la creaci\u00f3n y carga de datos en la base de datos. providers : Contiene los servicios que pueden inyectarse en los controladores u otros modulos de forma global. start : Contiene los archivos que se utilizan al momento de levantarse la aplicaci\u00f3n, ahi se puede configurar los services providers necesarios, registrar nuevos middlewares o plugins que se desee utilizar, etc. test : Contiene las carpetas de test unit o functional . ace : Es un ejecutable que hace de linea de comandos entre el AdonisJS y el proyecto, este archivo es propio del framework y no debe ser modificado. server.js : Archivo JS que se ejecuta para iniciar la aplicaci\u00f3n servidor. Como desarrollar con AdonisJS Crear un modelo P\u00e1gina de referencia Si desea crear un modelo, debe utilizar el comando: adonis make:model User Agregar en el modelo el siguiente metodo, como AdonisJS crea las tablas y por defecto le agrega las columnas created_at y updated_at exigiendo dichos datos, para evitar que el timestamp sea obligatorio se debe agregar el siguiente codigo: static boot() { super.boot(); this.addTrait('NoTimestamp'); } Crear una migraci\u00f3n Pagina de referencia Para crear la migraci\u00f3n, debe ejecutar el siguiente comando: adonis make:migration User o si desea crear la migraci\u00f3n junto al modelo puede utilizar el comando: adonis make:model User --migration El parametro --migration te permite crear ademas la migraci\u00f3n del modelo. El archivo se crea en la carpeta database/migrations , debe tener en cuenta que las migraciones se ejecutan en ORDEN, por lo que si posee foreing keys o referencias a otras tablas debe asegurarse que la migraci\u00f3n de la tabla a la que hace referencia sea creada con anterioridad. IMPORTANTE: Si no desea que el timestamp este presente como vimos en la creaci\u00f3n del modelo, elimine la linea que contiene el siguiente codigo: table.timestamps() . Crear un controlador P\u00e1gina de referencia El controlador hace de presenter y es quien realiza la l\u00f3gica de negocio de la aplicaci\u00f3n, para crear el controlador debe ejecutar el comando: adonis make:controller User AdonisJS le preguntar\u00e1 si desea que el controlador sea de tipo HTTP o WebSocket, seleccione la que desea crear. Existe un tipo de controlador llamado \"resource\" que ya dispone de los metodos listos para realizar un CRUD del modelo si asi lo desea, para ello utilice el comando: adonis make:controller User --resource El controlador se crea en la carpeta app/Controllers/Http (si eligio http) y en caso de que sea un controlador de tipo resource vera que en el archivo ya se agregaron los metodos del CRUD con sus respectivos comentarios (leerlos para saber que hacen y como se mapean con las rutas rest). Agregar la ruta HTTP Pagina de referencia Para agregar una nueva ruta, vaya al archivo start/routes.js y agregue la ruta que desea de la siguiente manera: Route.get('/users', 'UserController.index') El primer parametro del objecto Route es el nombre de la ruta (por lo general /algo) y el segundo parametro indica el nombre del controlador y el m\u00e9todo al que debe llamar cuando se realice la llamada HTTP con el m\u00e9todo que haya indicado en el objeto route (get, post, put o delete). Si desea realizar un mapeo de rutas con un controlador de tipo resource, puede definir la ruta de la siguiente manera: Route.resource('/users', 'UserController'); Esto autom\u00e1ticamente llamara a los m\u00e9todos del controlador que ya fueron creados y ademas realiza el mapeo de rutas dependiendo del tipo de llamada. Esto se hace para evitar crear rutas relacionadas a un CRUD y mapear una por una la ruta y el m\u00e9todo del controlador facilitando as\u00ed la definici\u00f3n de rutas y controladores en una sola linea. // Esto... Route.resource('users', 'UserController') // ...es equivalente a esto: Route.get('users', 'UserController.index') Route.post('users', 'UserController.store') Route.get('users/:id', 'UserController.show') Route.put('users/:id', 'UserController.update') Route.delete('users/:id', 'UserController.destroy') Tambien puede agrupar las rutas para que todas posean un prefijo: Route.group(() => { Route.resource('users', 'UserController') Route.resource('roles', 'RoleController'); }).prefix('api') Dando como resultado las rutas /api/users y /api/roles Si desea listar las rutas con sus controladores as\u00ed como informaci\u00f3n de utilidad puede utilizar el comando: adonis route:list","title":"Aplicaci\u00f3n servidor"},{"location":"aplicacion-servidor/#aplicacion-servidor","text":"A continuaci\u00f3n se le brindar\u00e1 informaci\u00f3n basica sobre como es el funcionamiento del framework y que debe hacer si desea agregar nuevas funcionalidades, esto es solo un vistazo por lo que se recomienda fuertemente que visite la documentaci\u00f3n de la pagina oficial si desea tener mas detalles. https://adonisjs.com/docs/4.1/about","title":"Aplicaci\u00f3n servidor"},{"location":"aplicacion-servidor/#estructura-del-servidor","text":"P\u00e1gina de referencia La estructura generada por AdonisJS consiste en los siguientes directorios proyecto \u251c\u2500\u2500 app \u251c\u2500\u2500 config \u251c\u2500\u2500 database \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 providers \u251c\u2500\u2500 start \u251c\u2500\u2500 test \u251c\u2500\u2500 .env \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 package.json \u251c\u2500\u2500 ace \u251c\u2500\u2500 server.js \u251c\u2500\u2500 vowfile.js \u251c\u2500\u2500 README.md A continuaci\u00f3n describiremos los directorios y archivos m\u00e1s importantes para que nos ayuden a entender m\u00e1s el funcionamiento del framework. app : Contiene los archivos necesarios para el funcionamiento basico de la aplicaci\u00f3n. A su vez contiene las carpetas: Controllers como presenter, Middleware en donde se encuentran los middlewares propios y Models en donde se encuentran los modelos asociados a las tablas de la base de datos. config : Como el nombre indica, contiene todos los archivos de configuracion de la aplicaci\u00f3n. database : Contiene los migration y seeds para la creaci\u00f3n y carga de datos en la base de datos. providers : Contiene los servicios que pueden inyectarse en los controladores u otros modulos de forma global. start : Contiene los archivos que se utilizan al momento de levantarse la aplicaci\u00f3n, ahi se puede configurar los services providers necesarios, registrar nuevos middlewares o plugins que se desee utilizar, etc. test : Contiene las carpetas de test unit o functional . ace : Es un ejecutable que hace de linea de comandos entre el AdonisJS y el proyecto, este archivo es propio del framework y no debe ser modificado. server.js : Archivo JS que se ejecuta para iniciar la aplicaci\u00f3n servidor.","title":"Estructura del servidor"},{"location":"aplicacion-servidor/#como-desarrollar-con-adonisjs","text":"","title":"Como desarrollar con AdonisJS"},{"location":"aplicacion-servidor/#crear-un-modelo","text":"P\u00e1gina de referencia Si desea crear un modelo, debe utilizar el comando: adonis make:model User Agregar en el modelo el siguiente metodo, como AdonisJS crea las tablas y por defecto le agrega las columnas created_at y updated_at exigiendo dichos datos, para evitar que el timestamp sea obligatorio se debe agregar el siguiente codigo: static boot() { super.boot(); this.addTrait('NoTimestamp'); }","title":"Crear un modelo"},{"location":"aplicacion-servidor/#crear-una-migracion","text":"Pagina de referencia Para crear la migraci\u00f3n, debe ejecutar el siguiente comando: adonis make:migration User o si desea crear la migraci\u00f3n junto al modelo puede utilizar el comando: adonis make:model User --migration El parametro --migration te permite crear ademas la migraci\u00f3n del modelo. El archivo se crea en la carpeta database/migrations , debe tener en cuenta que las migraciones se ejecutan en ORDEN, por lo que si posee foreing keys o referencias a otras tablas debe asegurarse que la migraci\u00f3n de la tabla a la que hace referencia sea creada con anterioridad. IMPORTANTE: Si no desea que el timestamp este presente como vimos en la creaci\u00f3n del modelo, elimine la linea que contiene el siguiente codigo: table.timestamps() .","title":"Crear una migraci\u00f3n"},{"location":"aplicacion-servidor/#crear-un-controlador","text":"P\u00e1gina de referencia El controlador hace de presenter y es quien realiza la l\u00f3gica de negocio de la aplicaci\u00f3n, para crear el controlador debe ejecutar el comando: adonis make:controller User AdonisJS le preguntar\u00e1 si desea que el controlador sea de tipo HTTP o WebSocket, seleccione la que desea crear. Existe un tipo de controlador llamado \"resource\" que ya dispone de los metodos listos para realizar un CRUD del modelo si asi lo desea, para ello utilice el comando: adonis make:controller User --resource El controlador se crea en la carpeta app/Controllers/Http (si eligio http) y en caso de que sea un controlador de tipo resource vera que en el archivo ya se agregaron los metodos del CRUD con sus respectivos comentarios (leerlos para saber que hacen y como se mapean con las rutas rest).","title":"Crear un controlador"},{"location":"aplicacion-servidor/#agregar-la-ruta-http","text":"Pagina de referencia Para agregar una nueva ruta, vaya al archivo start/routes.js y agregue la ruta que desea de la siguiente manera: Route.get('/users', 'UserController.index') El primer parametro del objecto Route es el nombre de la ruta (por lo general /algo) y el segundo parametro indica el nombre del controlador y el m\u00e9todo al que debe llamar cuando se realice la llamada HTTP con el m\u00e9todo que haya indicado en el objeto route (get, post, put o delete). Si desea realizar un mapeo de rutas con un controlador de tipo resource, puede definir la ruta de la siguiente manera: Route.resource('/users', 'UserController'); Esto autom\u00e1ticamente llamara a los m\u00e9todos del controlador que ya fueron creados y ademas realiza el mapeo de rutas dependiendo del tipo de llamada. Esto se hace para evitar crear rutas relacionadas a un CRUD y mapear una por una la ruta y el m\u00e9todo del controlador facilitando as\u00ed la definici\u00f3n de rutas y controladores en una sola linea. // Esto... Route.resource('users', 'UserController') // ...es equivalente a esto: Route.get('users', 'UserController.index') Route.post('users', 'UserController.store') Route.get('users/:id', 'UserController.show') Route.put('users/:id', 'UserController.update') Route.delete('users/:id', 'UserController.destroy') Tambien puede agrupar las rutas para que todas posean un prefijo: Route.group(() => { Route.resource('users', 'UserController') Route.resource('roles', 'RoleController'); }).prefix('api') Dando como resultado las rutas /api/users y /api/roles Si desea listar las rutas con sus controladores as\u00ed como informaci\u00f3n de utilidad puede utilizar el comando: adonis route:list","title":"Agregar la ruta HTTP"},{"location":"arquitectura/","text":"Arquitectura Introducci\u00f3n La arquitectura elegida es acorde a un esquema de micro-servicios en donde cada componente se encarga de realizar un trabajo especifico. A continuaci\u00f3n se presentar\u00e1 la arquitectura general y las arquitecturas de cada componente que forma parte del sistema y como se comunican entre si mediante el uso de colas de mensajer\u00eda. Arquitectura General El sistema esta compuesto por los siguientes componentes y v\u00edas de comunicaci\u00f3n, la decisi\u00f3n de esta arquitectura fue para permitir agilizar el procesamiento de la extracci\u00f3n de los art\u00edculos repartiendo la carga de trabajo y utilizando un sistema de colas de mensajer\u00eda que act\u00faa como deposito de \"informaci\u00f3n\" permitiendo a cada componente funcionar de manera independiente y actuar cuando los datos est\u00e9n disponibles. Los componentes del diagrama son: client : Aplicaci\u00f3n web que hace de interfaz entre el usuario y el sistema, se encarga de obtener del servidor los datos y presentarlos de manera visual en la pantalla para el uso de los usuarios finales. server : Componente encargado de enviar las ecuaciones de b\u00fasqueda a la cola de mensajer\u00eda y de recibir los art\u00edculos extra\u00eddos para persistir en la base de datos. Entre otras tareas tambi\u00e9n se encarga de brindar una API HTTP el cual es utilizado por el cliente web para la obtenci\u00f3n y persistencia de los datos necesarios. search-engine : Componente encargado de recibir las ecuaciones de b\u00fasqueda y ejecutar dichas b\u00fasquedas utilizando la API de Google Custom Search. Dicho componente se encuentra conectado a la cola de mensajer\u00eda RabbitMQ para recibir las ecuaciones y dejar los resultados de b\u00fasqueda. crawl-extractors : Componente encargado de recibir los resultados de b\u00fasqueda previamente mencionados, extraer el HTML en crudo y obtener los textos del articulo haciendo uso de selectores css, los datos se agregan a los resultados de b\u00fasqueda y se dejan en la cola de mensajer\u00eda para ser obtenidos por la aplicaci\u00f3n servidor. rabbitmq : Este componente es el mas importante debido a que es el eje central en las comunicaciones de los dem\u00e1s componentes, RabbitMQ permite la creaci\u00f3n de m\u00faltiples colas en donde se pueden intercambiar mensajes en formato de JSON facilitando la comunicaci\u00f3n y coordinaci\u00f3n entre los componentes. NLP : Este componente se encuentra separado de los dem\u00e1s sin hacer uso de las colas de mensajer\u00eda, su funci\u00f3n es la de obtener los art\u00edculos de la base de datos y realizar una serie de normalizaciones en el texto como eliminar signos, puntuaciones, n\u00fameros, acentos, stopwords, preposiciones, palabras especificas, etc. Dicho procesamiento se realiza para poder en un paso posterior construir una nube de palabras de todos los art\u00edculos normalizados.","title":"Arquitectura"},{"location":"arquitectura/#arquitectura","text":"","title":"Arquitectura"},{"location":"arquitectura/#introduccion","text":"La arquitectura elegida es acorde a un esquema de micro-servicios en donde cada componente se encarga de realizar un trabajo especifico. A continuaci\u00f3n se presentar\u00e1 la arquitectura general y las arquitecturas de cada componente que forma parte del sistema y como se comunican entre si mediante el uso de colas de mensajer\u00eda.","title":"Introducci\u00f3n"},{"location":"arquitectura/#arquitectura-general","text":"El sistema esta compuesto por los siguientes componentes y v\u00edas de comunicaci\u00f3n, la decisi\u00f3n de esta arquitectura fue para permitir agilizar el procesamiento de la extracci\u00f3n de los art\u00edculos repartiendo la carga de trabajo y utilizando un sistema de colas de mensajer\u00eda que act\u00faa como deposito de \"informaci\u00f3n\" permitiendo a cada componente funcionar de manera independiente y actuar cuando los datos est\u00e9n disponibles. Los componentes del diagrama son: client : Aplicaci\u00f3n web que hace de interfaz entre el usuario y el sistema, se encarga de obtener del servidor los datos y presentarlos de manera visual en la pantalla para el uso de los usuarios finales. server : Componente encargado de enviar las ecuaciones de b\u00fasqueda a la cola de mensajer\u00eda y de recibir los art\u00edculos extra\u00eddos para persistir en la base de datos. Entre otras tareas tambi\u00e9n se encarga de brindar una API HTTP el cual es utilizado por el cliente web para la obtenci\u00f3n y persistencia de los datos necesarios. search-engine : Componente encargado de recibir las ecuaciones de b\u00fasqueda y ejecutar dichas b\u00fasquedas utilizando la API de Google Custom Search. Dicho componente se encuentra conectado a la cola de mensajer\u00eda RabbitMQ para recibir las ecuaciones y dejar los resultados de b\u00fasqueda. crawl-extractors : Componente encargado de recibir los resultados de b\u00fasqueda previamente mencionados, extraer el HTML en crudo y obtener los textos del articulo haciendo uso de selectores css, los datos se agregan a los resultados de b\u00fasqueda y se dejan en la cola de mensajer\u00eda para ser obtenidos por la aplicaci\u00f3n servidor. rabbitmq : Este componente es el mas importante debido a que es el eje central en las comunicaciones de los dem\u00e1s componentes, RabbitMQ permite la creaci\u00f3n de m\u00faltiples colas en donde se pueden intercambiar mensajes en formato de JSON facilitando la comunicaci\u00f3n y coordinaci\u00f3n entre los componentes. NLP : Este componente se encuentra separado de los dem\u00e1s sin hacer uso de las colas de mensajer\u00eda, su funci\u00f3n es la de obtener los art\u00edculos de la base de datos y realizar una serie de normalizaciones en el texto como eliminar signos, puntuaciones, n\u00fameros, acentos, stopwords, preposiciones, palabras especificas, etc. Dicho procesamiento se realiza para poder en un paso posterior construir una nube de palabras de todos los art\u00edculos normalizados.","title":"Arquitectura General"},{"location":"herramientas-desarrollo/","text":"Herramientas de desarrollo El desarrollo del sistema se realiz\u00f3 con las siguientes tecnolog\u00edas: NodeJS 12.16.3 NPM 6.14.4 AdonisJS 4.0.12 VueJS 4.3.1 RabbitMQ 3.8.2 Control de versiones: github Editor de codigo: Visual Studio Code + plugins PostgresSQL 12.4 DBeaver (cliente de postgres) 6.2.0 Sistema operativo: Ubuntu desktop 20.4 64-bits","title":"Herramientas de desarrollo"},{"location":"herramientas-desarrollo/#herramientas-de-desarrollo","text":"El desarrollo del sistema se realiz\u00f3 con las siguientes tecnolog\u00edas: NodeJS 12.16.3 NPM 6.14.4 AdonisJS 4.0.12 VueJS 4.3.1 RabbitMQ 3.8.2 Control de versiones: github Editor de codigo: Visual Studio Code + plugins PostgresSQL 12.4 DBeaver (cliente de postgres) 6.2.0 Sistema operativo: Ubuntu desktop 20.4 64-bits","title":"Herramientas de desarrollo"},{"location":"instalacion/","text":"Instalaci\u00f3n y despliegue Requisitos previos NodeJS 10.X o superior P\u00e1gina de descarga RabbitMQ P\u00e1gina de descarga PostgreSQL P\u00e1gina de descarga AdonisJS P\u00e1gina de descarga VueJS P\u00e1gina de descarga (Opcional) forever: npm install -g forever Instalar Python 3 El proyecto hace uso de python 3.8 para el funcionamiento del NLP, para ello deber\u00e1 instalarlo en su sistema. Instalar python 3: apt-get install python3.8 Instalar pip: apt-get install python3-pip Actualizar pip: python3 -m pip install --upgrade pip Instalar virtualenv: pip3 install virtualenv En la carpeta nlp crear el virtualenv con el comando: virtualenv venv En caso de error \"setuptools pip failed with error code error\" ejecutar: pip3 install --upgrade setuptools (Opcional) Instalar spacy: pip3 install spacy (Opcional) Instalar el modelo en espa\u00f1ol de spacy: python -m spacy download es_core_news_lg Configurar Google CSE El sistema hace uso del buscador personalizado de Google, para ello deber\u00e1 crear un buscador en https://cse.google.com/cse/all . Al crear un buscador, Google le brindar\u00e1 un ID de buscador \u00fanico junto con su key necesarios para poder hacer uso del mismo desde llamadas http. Dichos datos deben ser incorporados en el .env de search-engine para poder ejecutar las b\u00fasquedas. Al acceder a la configuraci\u00f3n del buscador, podr\u00e1 ver varios par\u00e1metros como el nombre, descripci\u00f3n, idioma, etc. Una configuraci\u00f3n muy importante es incorporar los sitios web en donde se realizaran las b\u00fasquedas para que as\u00ed se pueda pre-filtrar los resultados por sitios espec\u00edficos. IMPORTANTE: si usa la versi\u00f3n gratuita, solo podr\u00e1 realizar 100 b\u00fasquedas por d\u00eda, en cambio si posee la versi\u00f3n paga, podr\u00e1 realizar una mayor cantidad de b\u00fasquedas. Este factor es importante ya que determina cual es el limite de b\u00fasquedas diarias que deber\u00e1 configurar en la base de datos (ver las siguientes secciones sobre como se configura). Actualmente los sitios con los que se trabaja son: https://diariocronica.com.ar https://www.eldiarioweb.com https://www.diariojornada.com.ar https://www.elpatagonico.com https://www.elchubut.com.ar https://radio3cadenapatagonia.com.ar https://diariolaportada.com.ar https://www.red43.com.ar Configurar RabbitMQ Las aplicaciones hacen uso de RabbitMQ para comunicarse entre si, para ello deber\u00e1 utilizar los siguientes comandos (quiz\u00e1s como root): Crear el virtualhost tesina con el comando: rabbitmqctl add_vhost tesina Verificar que efectivamente el virtualhost fue creado con: rabbitmqctl list_vhosts Agregar el usuario de RabbitMQ por defecto al virtualhost con todos los permisos: rabbitmqctl set_permissions -p tesina guest \".*\" \".*\" \".*\" Instalaci\u00f3n Clonar el proyecto https://github.com/emanuelbalcazar/tesina.git . Moverse a la carpeta raiz del proyecto cd tesina . Ejecutar el comando ./tesina.sh install para instalar las dependencias de nodejs en todas las aplicaciones. En el caso del nlp es diferente, debera moverse a la carpeta nlp y ejecutar los siguientes comandos: Activar el virtualenv con: source venv/bin/activate Instalar las dependencias con: pip install -r requirements.txt Descargar las librerias y corpus necesarios con: python download.py Configurar archivos .env Crear el archivo .env en las carpetas server , client , search-engine y crawl-extractors , utilizar el archivo .env.example como ejemplo completando los datos indicados incluyendo los parametros de conexi\u00f3n a la base de datos, conexi\u00f3n a RabbitMQ y claves de Google CSE previamente obtenidas. Si todos los datos fueron completados, las aplicaciones deber\u00edan poder ejecutarse sin problemas. Configurar los par\u00e1metros de conexi\u00f3n del NLP en nlp/configuration.ini modificando los datos que sean necesarios para permitir la conexi\u00f3n a la base de datos. Ejecutar el comando ./tesina.sh migrate para crear las tablas en la base de datos (asegurarse de que los .env sean correctos). Ejecutar el comando ./tesina.sh seed para cargar la base de datos con los datos iniciales necesarios para el funcionamiento de las aplicaciones. Despliegue Ejecutar el comando ./tesina.sh forever:startall para levantar todas las aplicaciones con forever. (Opcional) si no desea usar forever, puede ejecutar npm start en las carpetas crawl-extractors , search-engine , server y client . Para ejecutar el proceso de NLP ver en la siguiente secci\u00f3n. NLP El modulo de procesamiento de lenguaje general funciona de manera separada al resto de los componentes ya que no se integra al circuito de RabbitMQ. Para ejecutarlo debera hacer los siguientes pasos: Moverse a la carpeta nlp: cd nlp Activar el virtualenv: source venv/bin/activate Ejecutar el modulo principal con: python main.py El proceso comenzar\u00e1 a recuperar los art\u00edculos que NO fueron procesados desde la base de datos y los ira normalizando y persistiendo en la tabla de normalized_articles en la base de datos. Cada fila insertada contiene el texto resultante de la normalizaci\u00f3n as\u00ed como los pasos intermedios en cada columna. Al finalizar el proceso deber\u00eda tener la misma cantidad de art\u00edculos normalizados como art\u00edculos extra\u00eddos en su base de datos. AVISO : este proceso puede demorar bastante tiempo, se normalizan aproximadamente 10 art\u00edculos por minuto. Usando screen Probablemente, usted quiera dejar ejecutando el proceso en background, para ello utilizaremos screen. Instalar screen: apt-get install screen Moverse a la carpeta de NLP: cd nlp Escribir el comando screen: screen para entrar en la consola de screen. Activar el entorno virtual con: source venv/bin/activate Ejecutar el archivo main: python main.py Salir de screen presionando las teclas: ctrl + A + D al mismo tiempo, a partir de este punto puede cerrar su consola principal. Escriba el comando: screen -ls para comprobar que la consola de screen quedo en background Si desea retomar la consola, escriba: screen -r Verificaci\u00f3n Para verificar si las aplicaciones est\u00e1n ejecutando, acceda desde el navegador a las siguientes rutas por defecto de las aplicaciones para comprobar si su api rest se encuentra activa. Server: http://localhost:8000/ Search Engine: http://localhost:8001/api/ Crawl Extractors: http://localhost:8002/api/ Client: http://localhost:8080 En cualquier caso, las aplicaciones mostraran un mensaje al comenzar su ejecuci\u00f3n ya sea indicando que workers est\u00e1n activos o en que puerto estan escuchando, esta es otra manera de verificar que las aplicaciones est\u00e1n ejecutando y lograron conectarse a la base de datos y RabbitMQ. Configuraciones adicionales Al seguir todos los pasos previos, el servidor crear\u00e1 en la base de datos una tabla de configuraciones relacionadas con el planificador y los request limite para los workers, as\u00ed como la cantidad de workers disponibles. Estas configuraciones se pueden modificar si lo desea accediendo con el script y comando ./tesina.sh config en donde puede configurar: [requestCount] - Cantidad de request realizados en el dia - un contador que muestra cuantos request se van realizando entre todos los workers, se resetea cuando todos los workers cumplen con su cuota diaria de requests, esta configuraci\u00f3n no es necesario que sea modificada. [requestLimit] - Cantidad de request limite por dia - utilice esta configuraci\u00f3n para setear cuantos request se pueden realizar por cada d\u00eda, tenga en cuenta es este valor se divide en partes iguales entre todos los workers para definir la cuota de request de cada uno. [workers] - Cantidad de workers disponibles - por defecto ya incluye todos los workers del sistema, en caso de incorporar nuevos workers deber\u00e1 actualizar este valor. Esta configuraci\u00f3n es importante ya que es utilizada para dividir la cantidad de request por cada worker por lo que su valor debe ser siempre el correcto. [scheduleOnStart] - \u00bfIniciar planificador al iniciar el servidor? - esta configuraci\u00f3n le permite habilitar o deshabilitar el planificador al iniciar el servidor. [scheduleEvery] - Periodicidad del planificador para el envi\u00f3 de ecuaciones - indica en forma de notaci\u00f3n crontab cada cuanto el planificador debe enviar las ecuaciones a RabbitMQ para ser ejecutadas, por defecto el planificador debe ejecutar una vez al d\u00eda cuando los workers hayan acabado su cuota diaria de requests. [nextDayCron] - Notacion Cron de ejecuci\u00f3n una vez por d\u00eda - notaci\u00f3n que indica en formato de crontab cuando es una ejecuci\u00f3n diaria, solo se guarda en caso de necesitar reemplazar el valor del [scheduleEvery] pero por el momento no se hace uso de esta configuraci\u00f3n. [wordcloudSchedulerOnStart] - \u00bfIniciar el planificador de construcci\u00f3n de nubes de palabras? - indica si se debe ejecutar el planificador que levanta los articulos normalizados y los procesa para armar la nube de palabras. [wordcloudSchedulerEvery] - Periodicidad del constructor de nubes de palabras - indica la periodicidad con la que se ejecutara el constructor de nube de palabras, por defecto se ejecuta cada 5 minutos procesando un m\u00e1ximo de 50 articulos.","title":"Instalaci\u00f3n y despliegue"},{"location":"instalacion/#instalacion-y-despliegue","text":"","title":"Instalaci\u00f3n y despliegue"},{"location":"instalacion/#requisitos-previos","text":"NodeJS 10.X o superior P\u00e1gina de descarga RabbitMQ P\u00e1gina de descarga PostgreSQL P\u00e1gina de descarga AdonisJS P\u00e1gina de descarga VueJS P\u00e1gina de descarga (Opcional) forever: npm install -g forever","title":"Requisitos previos"},{"location":"instalacion/#instalar-python-3","text":"El proyecto hace uso de python 3.8 para el funcionamiento del NLP, para ello deber\u00e1 instalarlo en su sistema. Instalar python 3: apt-get install python3.8 Instalar pip: apt-get install python3-pip Actualizar pip: python3 -m pip install --upgrade pip Instalar virtualenv: pip3 install virtualenv En la carpeta nlp crear el virtualenv con el comando: virtualenv venv En caso de error \"setuptools pip failed with error code error\" ejecutar: pip3 install --upgrade setuptools (Opcional) Instalar spacy: pip3 install spacy (Opcional) Instalar el modelo en espa\u00f1ol de spacy: python -m spacy download es_core_news_lg","title":"Instalar Python 3"},{"location":"instalacion/#configurar-google-cse","text":"El sistema hace uso del buscador personalizado de Google, para ello deber\u00e1 crear un buscador en https://cse.google.com/cse/all . Al crear un buscador, Google le brindar\u00e1 un ID de buscador \u00fanico junto con su key necesarios para poder hacer uso del mismo desde llamadas http. Dichos datos deben ser incorporados en el .env de search-engine para poder ejecutar las b\u00fasquedas. Al acceder a la configuraci\u00f3n del buscador, podr\u00e1 ver varios par\u00e1metros como el nombre, descripci\u00f3n, idioma, etc. Una configuraci\u00f3n muy importante es incorporar los sitios web en donde se realizaran las b\u00fasquedas para que as\u00ed se pueda pre-filtrar los resultados por sitios espec\u00edficos. IMPORTANTE: si usa la versi\u00f3n gratuita, solo podr\u00e1 realizar 100 b\u00fasquedas por d\u00eda, en cambio si posee la versi\u00f3n paga, podr\u00e1 realizar una mayor cantidad de b\u00fasquedas. Este factor es importante ya que determina cual es el limite de b\u00fasquedas diarias que deber\u00e1 configurar en la base de datos (ver las siguientes secciones sobre como se configura). Actualmente los sitios con los que se trabaja son: https://diariocronica.com.ar https://www.eldiarioweb.com https://www.diariojornada.com.ar https://www.elpatagonico.com https://www.elchubut.com.ar https://radio3cadenapatagonia.com.ar https://diariolaportada.com.ar https://www.red43.com.ar","title":"Configurar Google CSE"},{"location":"instalacion/#configurar-rabbitmq","text":"Las aplicaciones hacen uso de RabbitMQ para comunicarse entre si, para ello deber\u00e1 utilizar los siguientes comandos (quiz\u00e1s como root): Crear el virtualhost tesina con el comando: rabbitmqctl add_vhost tesina Verificar que efectivamente el virtualhost fue creado con: rabbitmqctl list_vhosts Agregar el usuario de RabbitMQ por defecto al virtualhost con todos los permisos: rabbitmqctl set_permissions -p tesina guest \".*\" \".*\" \".*\"","title":"Configurar RabbitMQ"},{"location":"instalacion/#instalacion","text":"Clonar el proyecto https://github.com/emanuelbalcazar/tesina.git . Moverse a la carpeta raiz del proyecto cd tesina . Ejecutar el comando ./tesina.sh install para instalar las dependencias de nodejs en todas las aplicaciones. En el caso del nlp es diferente, debera moverse a la carpeta nlp y ejecutar los siguientes comandos: Activar el virtualenv con: source venv/bin/activate Instalar las dependencias con: pip install -r requirements.txt Descargar las librerias y corpus necesarios con: python download.py Configurar archivos .env Crear el archivo .env en las carpetas server , client , search-engine y crawl-extractors , utilizar el archivo .env.example como ejemplo completando los datos indicados incluyendo los parametros de conexi\u00f3n a la base de datos, conexi\u00f3n a RabbitMQ y claves de Google CSE previamente obtenidas. Si todos los datos fueron completados, las aplicaciones deber\u00edan poder ejecutarse sin problemas. Configurar los par\u00e1metros de conexi\u00f3n del NLP en nlp/configuration.ini modificando los datos que sean necesarios para permitir la conexi\u00f3n a la base de datos. Ejecutar el comando ./tesina.sh migrate para crear las tablas en la base de datos (asegurarse de que los .env sean correctos). Ejecutar el comando ./tesina.sh seed para cargar la base de datos con los datos iniciales necesarios para el funcionamiento de las aplicaciones.","title":"Instalaci\u00f3n"},{"location":"instalacion/#despliegue","text":"Ejecutar el comando ./tesina.sh forever:startall para levantar todas las aplicaciones con forever. (Opcional) si no desea usar forever, puede ejecutar npm start en las carpetas crawl-extractors , search-engine , server y client . Para ejecutar el proceso de NLP ver en la siguiente secci\u00f3n.","title":"Despliegue"},{"location":"instalacion/#nlp","text":"El modulo de procesamiento de lenguaje general funciona de manera separada al resto de los componentes ya que no se integra al circuito de RabbitMQ. Para ejecutarlo debera hacer los siguientes pasos: Moverse a la carpeta nlp: cd nlp Activar el virtualenv: source venv/bin/activate Ejecutar el modulo principal con: python main.py El proceso comenzar\u00e1 a recuperar los art\u00edculos que NO fueron procesados desde la base de datos y los ira normalizando y persistiendo en la tabla de normalized_articles en la base de datos. Cada fila insertada contiene el texto resultante de la normalizaci\u00f3n as\u00ed como los pasos intermedios en cada columna. Al finalizar el proceso deber\u00eda tener la misma cantidad de art\u00edculos normalizados como art\u00edculos extra\u00eddos en su base de datos. AVISO : este proceso puede demorar bastante tiempo, se normalizan aproximadamente 10 art\u00edculos por minuto.","title":"NLP"},{"location":"instalacion/#usando-screen","text":"Probablemente, usted quiera dejar ejecutando el proceso en background, para ello utilizaremos screen. Instalar screen: apt-get install screen Moverse a la carpeta de NLP: cd nlp Escribir el comando screen: screen para entrar en la consola de screen. Activar el entorno virtual con: source venv/bin/activate Ejecutar el archivo main: python main.py Salir de screen presionando las teclas: ctrl + A + D al mismo tiempo, a partir de este punto puede cerrar su consola principal. Escriba el comando: screen -ls para comprobar que la consola de screen quedo en background Si desea retomar la consola, escriba: screen -r","title":"Usando screen"},{"location":"instalacion/#verificacion","text":"Para verificar si las aplicaciones est\u00e1n ejecutando, acceda desde el navegador a las siguientes rutas por defecto de las aplicaciones para comprobar si su api rest se encuentra activa. Server: http://localhost:8000/ Search Engine: http://localhost:8001/api/ Crawl Extractors: http://localhost:8002/api/ Client: http://localhost:8080 En cualquier caso, las aplicaciones mostraran un mensaje al comenzar su ejecuci\u00f3n ya sea indicando que workers est\u00e1n activos o en que puerto estan escuchando, esta es otra manera de verificar que las aplicaciones est\u00e1n ejecutando y lograron conectarse a la base de datos y RabbitMQ.","title":"Verificaci\u00f3n"},{"location":"instalacion/#configuraciones-adicionales","text":"Al seguir todos los pasos previos, el servidor crear\u00e1 en la base de datos una tabla de configuraciones relacionadas con el planificador y los request limite para los workers, as\u00ed como la cantidad de workers disponibles. Estas configuraciones se pueden modificar si lo desea accediendo con el script y comando ./tesina.sh config en donde puede configurar: [requestCount] - Cantidad de request realizados en el dia - un contador que muestra cuantos request se van realizando entre todos los workers, se resetea cuando todos los workers cumplen con su cuota diaria de requests, esta configuraci\u00f3n no es necesario que sea modificada. [requestLimit] - Cantidad de request limite por dia - utilice esta configuraci\u00f3n para setear cuantos request se pueden realizar por cada d\u00eda, tenga en cuenta es este valor se divide en partes iguales entre todos los workers para definir la cuota de request de cada uno. [workers] - Cantidad de workers disponibles - por defecto ya incluye todos los workers del sistema, en caso de incorporar nuevos workers deber\u00e1 actualizar este valor. Esta configuraci\u00f3n es importante ya que es utilizada para dividir la cantidad de request por cada worker por lo que su valor debe ser siempre el correcto. [scheduleOnStart] - \u00bfIniciar planificador al iniciar el servidor? - esta configuraci\u00f3n le permite habilitar o deshabilitar el planificador al iniciar el servidor. [scheduleEvery] - Periodicidad del planificador para el envi\u00f3 de ecuaciones - indica en forma de notaci\u00f3n crontab cada cuanto el planificador debe enviar las ecuaciones a RabbitMQ para ser ejecutadas, por defecto el planificador debe ejecutar una vez al d\u00eda cuando los workers hayan acabado su cuota diaria de requests. [nextDayCron] - Notacion Cron de ejecuci\u00f3n una vez por d\u00eda - notaci\u00f3n que indica en formato de crontab cuando es una ejecuci\u00f3n diaria, solo se guarda en caso de necesitar reemplazar el valor del [scheduleEvery] pero por el momento no se hace uso de esta configuraci\u00f3n. [wordcloudSchedulerOnStart] - \u00bfIniciar el planificador de construcci\u00f3n de nubes de palabras? - indica si se debe ejecutar el planificador que levanta los articulos normalizados y los procesa para armar la nube de palabras. [wordcloudSchedulerEvery] - Periodicidad del constructor de nubes de palabras - indica la periodicidad con la que se ejecutara el constructor de nube de palabras, por defecto se ejecuta cada 5 minutos procesando un m\u00e1ximo de 50 articulos.","title":"Configuraciones adicionales"},{"location":"script-sh/","text":"Script SH Para facilitar el manejo de comandos, se incluyo un script llamado tesina.sh en la raiz del proyecto, dicho script agrupa una serie de comandos de utilidad necesarios para facilitar la instalaci\u00f3n y el uso del sistema. Los comandos disponibles son: ./tesina.sh install - instala las dependencias npm de todas las aplicaciones. ./tesina.sh migrate - crea las tablas en la base de datos segun esten configurados en server/database/migrations . ./tesina.sh rollback - deshace la ultima migraci\u00f3n ejecutada en la base de datos. ./tesina.sh seed - carga los datos iniciales en la base de datos para el setup inicial del sistema. ./tesina.sh config - muestra en la linea de comandos las opciones disponibles de configuraci\u00f3n del sistema, como periodicidad del planificador, cantidad de request limite a Google, cantidad de workers disponibles, etc. ./tesina.sh forever:start <PATH> - ejecuta con forever el script indicado en el PATH. ./tesina.sh forever:startall - ejecuta todas las aplicaciones con forever. ./tesina.sh forever:stop <PATH> - detiene la ejecuci\u00f3n del script indicado en el PATH. ./tesina.sh forever:stopall - detiene todas las aplicaciones ejecutando con forever. ./tesina.sh forever:restart <PATH> - reinicia con forever la ejecuci\u00f3n del script indicado en el PATH. ./tesina.sh forever:restartall - reinicia con forever todas las aplicaciones que est\u00e9n ejecut\u00e1ndose con forever. ./tesina.sh forever:list - lista las aplicaciones que est\u00e9n ejecut\u00e1ndose con forever. ./tesina.sh forever:logs - muestra donde est\u00e1n los logs de las aplicaciones.","title":"Script SH"},{"location":"script-sh/#script-sh","text":"Para facilitar el manejo de comandos, se incluyo un script llamado tesina.sh en la raiz del proyecto, dicho script agrupa una serie de comandos de utilidad necesarios para facilitar la instalaci\u00f3n y el uso del sistema. Los comandos disponibles son: ./tesina.sh install - instala las dependencias npm de todas las aplicaciones. ./tesina.sh migrate - crea las tablas en la base de datos segun esten configurados en server/database/migrations . ./tesina.sh rollback - deshace la ultima migraci\u00f3n ejecutada en la base de datos. ./tesina.sh seed - carga los datos iniciales en la base de datos para el setup inicial del sistema. ./tesina.sh config - muestra en la linea de comandos las opciones disponibles de configuraci\u00f3n del sistema, como periodicidad del planificador, cantidad de request limite a Google, cantidad de workers disponibles, etc. ./tesina.sh forever:start <PATH> - ejecuta con forever el script indicado en el PATH. ./tesina.sh forever:startall - ejecuta todas las aplicaciones con forever. ./tesina.sh forever:stop <PATH> - detiene la ejecuci\u00f3n del script indicado en el PATH. ./tesina.sh forever:stopall - detiene todas las aplicaciones ejecutando con forever. ./tesina.sh forever:restart <PATH> - reinicia con forever la ejecuci\u00f3n del script indicado en el PATH. ./tesina.sh forever:restartall - reinicia con forever todas las aplicaciones que est\u00e9n ejecut\u00e1ndose con forever. ./tesina.sh forever:list - lista las aplicaciones que est\u00e9n ejecut\u00e1ndose con forever. ./tesina.sh forever:logs - muestra donde est\u00e1n los logs de las aplicaciones.","title":"Script SH"}]}