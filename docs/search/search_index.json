{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fundamentaci\u00f3n El presente proyecto de tesina se enmarca en el proyecto \u201cAn\u00e1lisis Prospectivo Inteligente Del Impacto Social, Econ\u00f3mico Y Productivo Del Covid-19 En La Provincia De Chubut\u201d, el cual fue presentado y aprobado en la convocatoria PROGRAMA DE ARTICULACI\u00d3N Y FORTALECIMIENTO FEDERAL DE LAS CAPACIDADES EN CIENCIA Y TECNOLOG\u00cdA COVID-19. En este marco, el proyecto de tesina aborda una tem\u00e1tica particular y espec\u00edfica dentro del mencionado proyecto, la cual se fundamenta a continuaci\u00f3n. La vida de las personas y el curso de las actividades y servicios no esenciales han sufrido cambios dr\u00e1sticos a partir del Decreto DECNU-2020-297-APN-PTE y sus normativas anexas de nivel provincial y/o municipal en Chubut. Las medidas de Aislamiento Social, Preventivo y Obligatorio (ASPO) han generado y continuar\u00e1n generando desencadenantes de impacto social y econ\u00f3mico que necesitan ser identificados y monitoreados para proveer informaci\u00f3n a los formuladores de pol\u00edticas p\u00fablicas. Si bien existen herramientas y emprendimientos a nivel nacional, la provincia de Chubut necesita fortalecer el conjunto de instrumentos que sean capaces de determinar el c\u00famulo de variables involucradas, medirlas y transformarlas en indicadores cuantitativos y cualitativos que aporten a un tablero de an\u00e1lisis de impacto provincial. Este proyecto propone la construcci\u00f3n de conocimiento a partir de diferentes estrategias y herramientas de relevamiento de informaci\u00f3n y datos, que posibiliten un an\u00e1lisis, procesamiento y ponderaci\u00f3n de la situaci\u00f3n actual, as\u00ed como una predicci\u00f3n futura del estado de un territorio en t\u00e9rminos sociales, econ\u00f3micos y productivos, en virtud de una circunstancia sanitaria como la generada por el COVID-19. En particular se considerar\u00e1 la regi\u00f3n delimitada por los l\u00edmites geogr\u00e1ficos de la provincia del Chubut, ajustando la escala territorial a nivel de ciudades, pueblos y comunas rurales. La construcci\u00f3n de conocimiento, se har\u00e1 a partir de la extracci\u00f3n, procesamiento y an\u00e1lisis autom\u00e1tico de informaci\u00f3n period\u00edstica publicada en la prensa provincial, la cual luego ser\u00e1 presentada de manera acorde, a fin de poder evaluarse de manera indirecta la evoluci\u00f3n de diferentes t\u00f3picos, que impactan en la sociedad. Los resultados ser\u00e1n obtenidos mediante la aplicaci\u00f3n de t\u00e9cnicas de Procesamiento de Lenguaje Natural (NLP) y extracci\u00f3n de datos de sitios web (web scraping), para luego ser presentados en una aplicaci\u00f3n web que permitir\u00e1 su an\u00e1lisis y/o difusi\u00f3n. Antecedentes El alumno Emanuel Balcazar ha trabajado, bajo la direcci\u00f3n del Dr. Ing. Ordinez, en un sistema de recopilaci\u00f3n, procesamiento y visualizaci\u00f3n de profesionales de Chubut, denominado \u201cGeoPerfil Profesional\u201d, el cual fue presentado, aprobado y financiado por la convocatoria Universidades Agregado Valor 2017 de la Secretar\u00eda de Pol\u00edticas Universitarias. En dicho proyecto, se utilizaron m\u00e9todos de extracci\u00f3n de datos de diversas fuentes (scrapping) y procesamiento del lenguaje natural (NLP) para llevar a cabo el proyecto, adem\u00e1s de implementar una aplicaci\u00f3n web para visualizar los resultados. Objetivos Objetivo General Construcci\u00f3n de conocimiento acerca de la situaci\u00f3n de la provincia del Chubut en el marco de la pandemia por COVID-19 mediante la extracci\u00f3n y el procesamiento autom\u00e1tico de informaci\u00f3n de fuentes period\u00edsticas sobre t\u00f3picos afectados por el contexto sanitario: econom\u00eda, educaci\u00f3n, movilidad, producci\u00f3n, vulnerabilidad. Objetivos Especificos Investigar y recopilar la informaci\u00f3n disponible, as\u00ed como herramientas de trabajo y proyectos similares. Identificar y caracterizar fuentes period\u00edsticas de la provincia del Chubut en t\u00e9rminos de extracci\u00f3n autom\u00e1tica de sus notas. Implementar un vocabulario que sintetice, mediante un conjunto de palabras, un t\u00f3pico particular. Generar una herramienta de extracci\u00f3n autom\u00e1tica de informaci\u00f3n de fuentes period\u00edsticas. Analizar la informaci\u00f3n extra\u00edda mediante t\u00e9cnicas de Procesamiento de Lenguaje Natural que permitan generar nueva informaci\u00f3n y/o representarla de manera accesible. Mostrar los resultados utilizando diversas herramientas inform\u00e1ticas (como gr\u00e1ficos, mapas, etc) que sinteticen los resultados obtenidos. Desarrollo Propuesto Las fuentes de informaci\u00f3n abiertas de donde se obtendr\u00e1n los datos son p\u00e1ginas de diarios y noticias online en donde se mencionan ciertos t\u00f3picos (econom\u00eda, salud, sentimientos, etc) relacionados con la reciente pandemia. En el aspecto tecnol\u00f3gico se evaluaran las herramientas a utilizar, siendo \u00e9stas frameworks que permitan implementar los procesos que se necesiten para generar los resultados. Tambi\u00e9n se tendr\u00e1n en cuenta las herramientas de representaci\u00f3n de datos como mapas, gr\u00e1ficos estad\u00edsticos u otros que sean de inter\u00e9s para sintetizar la informaci\u00f3n resultante y que permitan en todos los casos analizar la evoluci\u00f3n temporal de los contenidos extra\u00eddos. Se utilizaran t\u00e9cnicas de procesamiento de lenguaje natural para interpretar y generar informaci\u00f3n a partir de los datos que se obtengan durante la fase de extracci\u00f3n de art\u00edculos, lo cual se realizar\u00e1 de manera autom\u00e1tica de tal forma que continuamente se disponga de nueva informaci\u00f3n. Se podr\u00e1n hacer uso de librer\u00edas gr\u00e1ficas para representar la informaci\u00f3n con mapas interactivos, gr\u00e1ficos estad\u00edsticos y todo lo que se requiera para lograr un resultado accesible y legible para el p\u00fablico. Como complemento se analizar\u00e1 la factibilidad de hacer uso del \u201can\u00e1lisis de sentimientos\u201d sobre la opini\u00f3n de la prensa respecto a c\u00f3mo ha afectado la pandemia y el permanecer durante tanto tiempo en cuarentena permitiendo conocer un poco m\u00e1s el impacto social en Chubut. Adem\u00e1s se podr\u00eda construir, a partir de las emociones reflejadas en las notas, un gr\u00e1fico que muestre por zona en que lugares a impactado de manera m\u00e1s negativa la pandemia. Estos aspectos ser\u00e1n primeramente analizados en cuanto a su factibilidad y potencial de representatividad estad\u00edstica. Resultados Esperados Se esperan alcanzar los siguientes hitos, como parte del proceso pedag\u00f3gico consistente en un desarrollo mayormente aut\u00f3nomo y de complejidad acorde un trabajo profesional. Amplitud, criterio y autonom\u00eda en la b\u00fasqueda de informaci\u00f3n, selecci\u00f3n de tecnolog\u00eda y dimensionamiento de la complejidad para un proyecto. Conocimiento de las problem\u00e1ticas y alternativas de soluci\u00f3n para una situaci\u00f3n difusamente planteada (realidad). Sistematizaci\u00f3n y organizaci\u00f3n de tareas, m\u00e9todos y materiales para alcanzar objetivos. Construcci\u00f3n de un sistema de extracci\u00f3n, an\u00e1lisis y visualizaci\u00f3n de informaci\u00f3n period\u00edstica del impacto en distintas dimensiones de la pandemia por COVID-19 en la provincia del Chubut, el cual se encuentre desplegado en un ambiente de producci\u00f3n. Cronograma de Actividades # Actividad Duraci\u00f3n Aprox. 1 Inmersi\u00f3n y estado del arte en extracci\u00f3n autom\u00e1tica y procesamiento de lenguaje natural de art\u00edculos period\u00edsticos 1 semana 2 Arquitectura la soluci\u00f3n y selecci\u00f3n de fuentes 1 semana 3 Estudio de vocabulario 2 semanas 4 Experimentaci\u00f3n con tecnolog\u00edas 2 semanas 5 Dise\u00f1o, implementaci\u00f3n y prueba de extractor 3 semanas 6 Dise\u00f1o, implementaci\u00f3n y prueba de analizador 3 semanas 7 Integraci\u00f3n, pruebas y correcciones de visualizador 3 semanas Total 15 semanas","title":"Principal"},{"location":"#fundamentacion","text":"El presente proyecto de tesina se enmarca en el proyecto \u201cAn\u00e1lisis Prospectivo Inteligente Del Impacto Social, Econ\u00f3mico Y Productivo Del Covid-19 En La Provincia De Chubut\u201d, el cual fue presentado y aprobado en la convocatoria PROGRAMA DE ARTICULACI\u00d3N Y FORTALECIMIENTO FEDERAL DE LAS CAPACIDADES EN CIENCIA Y TECNOLOG\u00cdA COVID-19. En este marco, el proyecto de tesina aborda una tem\u00e1tica particular y espec\u00edfica dentro del mencionado proyecto, la cual se fundamenta a continuaci\u00f3n. La vida de las personas y el curso de las actividades y servicios no esenciales han sufrido cambios dr\u00e1sticos a partir del Decreto DECNU-2020-297-APN-PTE y sus normativas anexas de nivel provincial y/o municipal en Chubut. Las medidas de Aislamiento Social, Preventivo y Obligatorio (ASPO) han generado y continuar\u00e1n generando desencadenantes de impacto social y econ\u00f3mico que necesitan ser identificados y monitoreados para proveer informaci\u00f3n a los formuladores de pol\u00edticas p\u00fablicas. Si bien existen herramientas y emprendimientos a nivel nacional, la provincia de Chubut necesita fortalecer el conjunto de instrumentos que sean capaces de determinar el c\u00famulo de variables involucradas, medirlas y transformarlas en indicadores cuantitativos y cualitativos que aporten a un tablero de an\u00e1lisis de impacto provincial. Este proyecto propone la construcci\u00f3n de conocimiento a partir de diferentes estrategias y herramientas de relevamiento de informaci\u00f3n y datos, que posibiliten un an\u00e1lisis, procesamiento y ponderaci\u00f3n de la situaci\u00f3n actual, as\u00ed como una predicci\u00f3n futura del estado de un territorio en t\u00e9rminos sociales, econ\u00f3micos y productivos, en virtud de una circunstancia sanitaria como la generada por el COVID-19. En particular se considerar\u00e1 la regi\u00f3n delimitada por los l\u00edmites geogr\u00e1ficos de la provincia del Chubut, ajustando la escala territorial a nivel de ciudades, pueblos y comunas rurales. La construcci\u00f3n de conocimiento, se har\u00e1 a partir de la extracci\u00f3n, procesamiento y an\u00e1lisis autom\u00e1tico de informaci\u00f3n period\u00edstica publicada en la prensa provincial, la cual luego ser\u00e1 presentada de manera acorde, a fin de poder evaluarse de manera indirecta la evoluci\u00f3n de diferentes t\u00f3picos, que impactan en la sociedad. Los resultados ser\u00e1n obtenidos mediante la aplicaci\u00f3n de t\u00e9cnicas de Procesamiento de Lenguaje Natural (NLP) y extracci\u00f3n de datos de sitios web (web scraping), para luego ser presentados en una aplicaci\u00f3n web que permitir\u00e1 su an\u00e1lisis y/o difusi\u00f3n.","title":"Fundamentaci\u00f3n"},{"location":"#antecedentes","text":"El alumno Emanuel Balcazar ha trabajado, bajo la direcci\u00f3n del Dr. Ing. Ordinez, en un sistema de recopilaci\u00f3n, procesamiento y visualizaci\u00f3n de profesionales de Chubut, denominado \u201cGeoPerfil Profesional\u201d, el cual fue presentado, aprobado y financiado por la convocatoria Universidades Agregado Valor 2017 de la Secretar\u00eda de Pol\u00edticas Universitarias. En dicho proyecto, se utilizaron m\u00e9todos de extracci\u00f3n de datos de diversas fuentes (scrapping) y procesamiento del lenguaje natural (NLP) para llevar a cabo el proyecto, adem\u00e1s de implementar una aplicaci\u00f3n web para visualizar los resultados.","title":"Antecedentes"},{"location":"#objetivos","text":"","title":"Objetivos"},{"location":"#objetivo-general","text":"Construcci\u00f3n de conocimiento acerca de la situaci\u00f3n de la provincia del Chubut en el marco de la pandemia por COVID-19 mediante la extracci\u00f3n y el procesamiento autom\u00e1tico de informaci\u00f3n de fuentes period\u00edsticas sobre t\u00f3picos afectados por el contexto sanitario: econom\u00eda, educaci\u00f3n, movilidad, producci\u00f3n, vulnerabilidad.","title":"Objetivo General"},{"location":"#objetivos-especificos","text":"Investigar y recopilar la informaci\u00f3n disponible, as\u00ed como herramientas de trabajo y proyectos similares. Identificar y caracterizar fuentes period\u00edsticas de la provincia del Chubut en t\u00e9rminos de extracci\u00f3n autom\u00e1tica de sus notas. Implementar un vocabulario que sintetice, mediante un conjunto de palabras, un t\u00f3pico particular. Generar una herramienta de extracci\u00f3n autom\u00e1tica de informaci\u00f3n de fuentes period\u00edsticas. Analizar la informaci\u00f3n extra\u00edda mediante t\u00e9cnicas de Procesamiento de Lenguaje Natural que permitan generar nueva informaci\u00f3n y/o representarla de manera accesible. Mostrar los resultados utilizando diversas herramientas inform\u00e1ticas (como gr\u00e1ficos, mapas, etc) que sinteticen los resultados obtenidos.","title":"Objetivos Especificos"},{"location":"#desarrollo-propuesto","text":"Las fuentes de informaci\u00f3n abiertas de donde se obtendr\u00e1n los datos son p\u00e1ginas de diarios y noticias online en donde se mencionan ciertos t\u00f3picos (econom\u00eda, salud, sentimientos, etc) relacionados con la reciente pandemia. En el aspecto tecnol\u00f3gico se evaluaran las herramientas a utilizar, siendo \u00e9stas frameworks que permitan implementar los procesos que se necesiten para generar los resultados. Tambi\u00e9n se tendr\u00e1n en cuenta las herramientas de representaci\u00f3n de datos como mapas, gr\u00e1ficos estad\u00edsticos u otros que sean de inter\u00e9s para sintetizar la informaci\u00f3n resultante y que permitan en todos los casos analizar la evoluci\u00f3n temporal de los contenidos extra\u00eddos. Se utilizaran t\u00e9cnicas de procesamiento de lenguaje natural para interpretar y generar informaci\u00f3n a partir de los datos que se obtengan durante la fase de extracci\u00f3n de art\u00edculos, lo cual se realizar\u00e1 de manera autom\u00e1tica de tal forma que continuamente se disponga de nueva informaci\u00f3n. Se podr\u00e1n hacer uso de librer\u00edas gr\u00e1ficas para representar la informaci\u00f3n con mapas interactivos, gr\u00e1ficos estad\u00edsticos y todo lo que se requiera para lograr un resultado accesible y legible para el p\u00fablico. Como complemento se analizar\u00e1 la factibilidad de hacer uso del \u201can\u00e1lisis de sentimientos\u201d sobre la opini\u00f3n de la prensa respecto a c\u00f3mo ha afectado la pandemia y el permanecer durante tanto tiempo en cuarentena permitiendo conocer un poco m\u00e1s el impacto social en Chubut. Adem\u00e1s se podr\u00eda construir, a partir de las emociones reflejadas en las notas, un gr\u00e1fico que muestre por zona en que lugares a impactado de manera m\u00e1s negativa la pandemia. Estos aspectos ser\u00e1n primeramente analizados en cuanto a su factibilidad y potencial de representatividad estad\u00edstica.","title":"Desarrollo Propuesto"},{"location":"#resultados-esperados","text":"Se esperan alcanzar los siguientes hitos, como parte del proceso pedag\u00f3gico consistente en un desarrollo mayormente aut\u00f3nomo y de complejidad acorde un trabajo profesional. Amplitud, criterio y autonom\u00eda en la b\u00fasqueda de informaci\u00f3n, selecci\u00f3n de tecnolog\u00eda y dimensionamiento de la complejidad para un proyecto. Conocimiento de las problem\u00e1ticas y alternativas de soluci\u00f3n para una situaci\u00f3n difusamente planteada (realidad). Sistematizaci\u00f3n y organizaci\u00f3n de tareas, m\u00e9todos y materiales para alcanzar objetivos. Construcci\u00f3n de un sistema de extracci\u00f3n, an\u00e1lisis y visualizaci\u00f3n de informaci\u00f3n period\u00edstica del impacto en distintas dimensiones de la pandemia por COVID-19 en la provincia del Chubut, el cual se encuentre desplegado en un ambiente de producci\u00f3n.","title":"Resultados Esperados"},{"location":"#cronograma-de-actividades","text":"# Actividad Duraci\u00f3n Aprox. 1 Inmersi\u00f3n y estado del arte en extracci\u00f3n autom\u00e1tica y procesamiento de lenguaje natural de art\u00edculos period\u00edsticos 1 semana 2 Arquitectura la soluci\u00f3n y selecci\u00f3n de fuentes 1 semana 3 Estudio de vocabulario 2 semanas 4 Experimentaci\u00f3n con tecnolog\u00edas 2 semanas 5 Dise\u00f1o, implementaci\u00f3n y prueba de extractor 3 semanas 6 Dise\u00f1o, implementaci\u00f3n y prueba de analizador 3 semanas 7 Integraci\u00f3n, pruebas y correcciones de visualizador 3 semanas Total 15 semanas","title":"Cronograma de Actividades"},{"location":"instalacion/","text":"Requisitos previos NodeJS 10.X o superior P\u00e1gina de descarga RabbitMQ P\u00e1gina de descarga PostgreSQL P\u00e1gina de descarga AdonisJS P\u00e1gina de descarga VueJS P\u00e1gina de descarga (Opcional) forever: npm install -g forever Instalar Python 3 El proyecto hace uso de python 3.8 para el funcionamiento del NLP, para ello deber\u00e1 instalarlo en su sistema. Instalar python 3: apt-get install python3.8 Instalar pip: apt-get install python3-pip Actualizar pip: python3 -m pip install --upgrade pip Instalar virtualenv: pip3 install virtualenv En la carpeta nlp crear el virtualenv con el comando: virtualenv venv En caso de error \"setuptools pip failed with error code error\" ejecutar: pip3 install --upgrade setuptools (Opcional) Instalar spacy: pip3 install spacy (Opcional) Instalar el modelo en espa\u00f1ol de spacy: python -m spacy download es_core_news_lg Configurar Google CSE El sistema hace uso del buscador personalizado de Google, para ello deber\u00e1 crear un buscador en https://cse.google.com/cse/all . Al crear un buscador, Google le brindar\u00e1 un ID de buscador \u00fanico junto con su key necesarios para poder hacer uso del mismo desde llamadas http. Dichos datos deben ser incorporados en el .env de search-engine para poder ejecutar las b\u00fasquedas. Al acceder a la configuraci\u00f3n del buscador, podr\u00e1 ver varios par\u00e1metros como el nombre, descripci\u00f3n, idioma, etc. Una configuraci\u00f3n muy importante es incorporar los sitios web en donde se realizaran las b\u00fasquedas para que as\u00ed se pueda pre-filtrar los resultados por sitios espec\u00edficos. IMPORTANTE: si usa la versi\u00f3n gratuita, solo podr\u00e1 realizar 100 b\u00fasquedas por d\u00eda, en cambio si posee la versi\u00f3n paga, podr\u00e1 realizar una mayor cantidad de b\u00fasquedas. Este factor es importante ya que determina cual es el limite de b\u00fasquedas diarias que deber\u00e1 configurar en la base de datos (ver las siguientes secciones sobre como se configura). Actualmente los sitios con los que se trabaja son: https://diariocronica.com.ar https://www.eldiarioweb.com https://www.diariojornada.com.ar https://www.elpatagonico.com https://www.elchubut.com.ar https://radio3cadenapatagonia.com.ar https://diariolaportada.com.ar https://www.red43.com.ar Configurar RabbitMQ Las aplicaciones hacen uso de RabbitMQ para comunicarse entre si, para ello deber\u00e1 utilizar los siguientes comandos (quiz\u00e1s como root): Crear el virtualhost tesina con el comando: rabbitmqctl add_vhost tesina Verificar que efectivamente el virtualhost fue creado con: rabbitmqctl list_vhosts Agregar el usuario de RabbitMQ por defecto al virtualhost con todos los permisos: rabbitmqctl set_permissions -p tesina guest \".*\" \".*\" \".*\" Instalaci\u00f3n Clonar el proyecto https://github.com/emanuelbalcazar/tesina.git . Moverse a la carpeta raiz del proyecto cd tesina . Ejecutar el comando ./tesina.sh install para instalar las dependencias de nodejs en todas las aplicaciones. En el caso del nlp es diferente, debera moverse a la carpeta nlp y ejecutar los siguientes comandos: Activar el virtualenv con: source venv/bin/activate Instalar las dependencias con: pip install -r requirements.txt Descargar las librerias y corpus necesarios con: python download.py Configurar archivos .env Crear el archivo .env en las carpetas server , client , search-engine y crawl-extractors , utilizar el archivo .env.example como ejemplo completando los datos indicados incluyendo los parametros de conexi\u00f3n a la base de datos, conexi\u00f3n a RabbitMQ y claves de Google CSE previamente obtenidas. Si todos los datos fueron completados, las aplicaciones deber\u00edan poder ejecutarse sin problemas. Configurar los par\u00e1metros de conexi\u00f3n del NLP en nlp/configuration.ini modificando los datos que sean necesarios para permitir la conexi\u00f3n a la base de datos. Ejecutar el comando ./tesina.sh migrate para crear las tablas en la base de datos (asegurarse de que los .env sean correctos). Ejecutar el comando ./tesina.sh seed para cargar la base de datos con los datos iniciales necesarios para el funcionamiento de las aplicaciones. Despliegue Ejecutar el comando ./tesina.sh forever:startall para levantar todas las aplicaciones con forever. (Opcional) si no desea usar forever, puede ejecutar npm start en las carpetas crawl-extractors , search-engine , server y client . Para ejecutar el proceso de NLP ver en la siguiente secci\u00f3n. NLP El modulo de procesamiento de lenguaje general funciona de manera separada al resto de los componentes ya que no se integra al circuito de RabbitMQ. Para ejecutarlo debera hacer los siguientes pasos: Moverse a la carpeta nlp: cd nlp Activar el virtualenv: source venv/bin/activate Ejecutar el modulo principal con: python main.py El proceso comenzar\u00e1 a recuperar los art\u00edculos que NO fueron procesados desde la base de datos y los ira normalizando y persistiendo en la tabla de normalized_articles en la base de datos. Cada fila insertada contiene el texto resultante de la normalizaci\u00f3n as\u00ed como los pasos intermedios en cada columna. Al finalizar el proceso deber\u00eda tener la misma cantidad de art\u00edculos normalizados como art\u00edculos extra\u00eddos en su base de datos. AVISO : este proceso puede demorar bastante tiempo, se normalizan aproximadamente 10 art\u00edculos por minuto. Usando screen Probablemente, usted quiera dejar ejecutando el proceso en background, para ello utilizaremos screen. Instalar screen: apt-get install screen Moverse a la carpeta de NLP: cd nlp Escribir el comando screen: screen para entrar en la consola de screen. Activar el entorno virtual con: source venv/bin/activate Ejecutar el archivo main: python main.py Salir de screen presionando las teclas: ctrl + A + D al mismo tiempo, a partir de este punto puede cerrar su consola principal. Escriba el comando: screen -ls para comprobar que la consola de screen quedo en background Si desea retomar la consola, escriba: screen -r Verificaci\u00f3n Para verificar si las aplicaciones est\u00e1n ejecutando, acceda desde el navegador a las siguientes rutas por defecto de las aplicaciones para comprobar si su api rest se encuentra activa. Server: http://localhost:8000/ Search Engine: http://localhost:8001/api/ Crawl Extractors: http://localhost:8002/api/ Client: http://localhost:8080 En cualquier caso, las aplicaciones mostraran un mensaje al comenzar su ejecuci\u00f3n ya sea indicando que workers est\u00e1n activos o en que puerto estan escuchando, esta es otra manera de verificar que las aplicaciones est\u00e1n ejecutando y lograron conectarse a la base de datos y RabbitMQ. Configuraciones adicionales Al seguir todos los pasos previos, el servidor crear\u00e1 en la base de datos una tabla de configuraciones relacionadas con el planificador y los request limite para los workers, as\u00ed como la cantidad de workers disponibles. Estas configuraciones se pueden modificar si lo desea accediendo con el script y comando ./tesina.sh config en donde puede configurar: [requestCount] - Cantidad de request realizados en el dia - un contador que muestra cuantos request se van realizando entre todos los workers, se resetea cuando todos los workers cumplen con su cuota diaria de requests, esta configuraci\u00f3n no es necesario que sea modificada. [requestLimit] - Cantidad de request limite por dia - utilice esta configuraci\u00f3n para setear cuantos request se pueden realizar por cada d\u00eda, tenga en cuenta es este valor se divide en partes iguales entre todos los workers para definir la cuota de request de cada uno. [workers] - Cantidad de workers disponibles - por defecto ya incluye todos los workers del sistema, en caso de incorporar nuevos workers deber\u00e1 actualizar este valor. Esta configuraci\u00f3n es importante ya que es utilizada para dividir la cantidad de request por cada worker por lo que su valor debe ser siempre el correcto. [scheduleOnStart] - \u00bfIniciar planificador al iniciar el servidor? - esta configuraci\u00f3n le permite habilitar o deshabilitar el planificador al iniciar el servidor. [scheduleEvery] - Periodicidad del planificador para el envi\u00f3 de ecuaciones - indica en forma de notaci\u00f3n crontab cada cuanto el planificador debe enviar las ecuaciones a RabbitMQ para ser ejecutadas, por defecto el planificador debe ejecutar una vez al d\u00eda cuando los workers hayan acabado su cuota diaria de requests. [nextDayCron] - Notacion Cron de ejecuci\u00f3n una vez por d\u00eda - notaci\u00f3n que indica en formato de crontab cuando es una ejecuci\u00f3n diaria, solo se guarda en caso de necesitar reemplazar el valor del [scheduleEvery] pero por el momento no se hace uso de esta configuraci\u00f3n. [wordcloudSchedulerOnStart] - \u00bfIniciar el planificador de construcci\u00f3n de nubes de palabras? - indica si se debe ejecutar el planificador que levanta los articulos normalizados y los procesa para armar la nube de palabras. [wordcloudSchedulerEvery] - Periodicidad del constructor de nubes de palabras - indica la periodicidad con la que se ejecutara el constructor de nube de palabras, por defecto se ejecuta cada 5 minutos procesando un m\u00e1ximo de 50 articulos.","title":"Instalaci\u00f3n y despliegue"},{"location":"instalacion/#requisitos-previos","text":"NodeJS 10.X o superior P\u00e1gina de descarga RabbitMQ P\u00e1gina de descarga PostgreSQL P\u00e1gina de descarga AdonisJS P\u00e1gina de descarga VueJS P\u00e1gina de descarga (Opcional) forever: npm install -g forever","title":"Requisitos previos"},{"location":"instalacion/#instalar-python-3","text":"El proyecto hace uso de python 3.8 para el funcionamiento del NLP, para ello deber\u00e1 instalarlo en su sistema. Instalar python 3: apt-get install python3.8 Instalar pip: apt-get install python3-pip Actualizar pip: python3 -m pip install --upgrade pip Instalar virtualenv: pip3 install virtualenv En la carpeta nlp crear el virtualenv con el comando: virtualenv venv En caso de error \"setuptools pip failed with error code error\" ejecutar: pip3 install --upgrade setuptools (Opcional) Instalar spacy: pip3 install spacy (Opcional) Instalar el modelo en espa\u00f1ol de spacy: python -m spacy download es_core_news_lg","title":"Instalar Python 3"},{"location":"instalacion/#configurar-google-cse","text":"El sistema hace uso del buscador personalizado de Google, para ello deber\u00e1 crear un buscador en https://cse.google.com/cse/all . Al crear un buscador, Google le brindar\u00e1 un ID de buscador \u00fanico junto con su key necesarios para poder hacer uso del mismo desde llamadas http. Dichos datos deben ser incorporados en el .env de search-engine para poder ejecutar las b\u00fasquedas. Al acceder a la configuraci\u00f3n del buscador, podr\u00e1 ver varios par\u00e1metros como el nombre, descripci\u00f3n, idioma, etc. Una configuraci\u00f3n muy importante es incorporar los sitios web en donde se realizaran las b\u00fasquedas para que as\u00ed se pueda pre-filtrar los resultados por sitios espec\u00edficos. IMPORTANTE: si usa la versi\u00f3n gratuita, solo podr\u00e1 realizar 100 b\u00fasquedas por d\u00eda, en cambio si posee la versi\u00f3n paga, podr\u00e1 realizar una mayor cantidad de b\u00fasquedas. Este factor es importante ya que determina cual es el limite de b\u00fasquedas diarias que deber\u00e1 configurar en la base de datos (ver las siguientes secciones sobre como se configura). Actualmente los sitios con los que se trabaja son: https://diariocronica.com.ar https://www.eldiarioweb.com https://www.diariojornada.com.ar https://www.elpatagonico.com https://www.elchubut.com.ar https://radio3cadenapatagonia.com.ar https://diariolaportada.com.ar https://www.red43.com.ar","title":"Configurar Google CSE"},{"location":"instalacion/#configurar-rabbitmq","text":"Las aplicaciones hacen uso de RabbitMQ para comunicarse entre si, para ello deber\u00e1 utilizar los siguientes comandos (quiz\u00e1s como root): Crear el virtualhost tesina con el comando: rabbitmqctl add_vhost tesina Verificar que efectivamente el virtualhost fue creado con: rabbitmqctl list_vhosts Agregar el usuario de RabbitMQ por defecto al virtualhost con todos los permisos: rabbitmqctl set_permissions -p tesina guest \".*\" \".*\" \".*\"","title":"Configurar RabbitMQ"},{"location":"instalacion/#instalacion","text":"Clonar el proyecto https://github.com/emanuelbalcazar/tesina.git . Moverse a la carpeta raiz del proyecto cd tesina . Ejecutar el comando ./tesina.sh install para instalar las dependencias de nodejs en todas las aplicaciones. En el caso del nlp es diferente, debera moverse a la carpeta nlp y ejecutar los siguientes comandos: Activar el virtualenv con: source venv/bin/activate Instalar las dependencias con: pip install -r requirements.txt Descargar las librerias y corpus necesarios con: python download.py Configurar archivos .env Crear el archivo .env en las carpetas server , client , search-engine y crawl-extractors , utilizar el archivo .env.example como ejemplo completando los datos indicados incluyendo los parametros de conexi\u00f3n a la base de datos, conexi\u00f3n a RabbitMQ y claves de Google CSE previamente obtenidas. Si todos los datos fueron completados, las aplicaciones deber\u00edan poder ejecutarse sin problemas. Configurar los par\u00e1metros de conexi\u00f3n del NLP en nlp/configuration.ini modificando los datos que sean necesarios para permitir la conexi\u00f3n a la base de datos. Ejecutar el comando ./tesina.sh migrate para crear las tablas en la base de datos (asegurarse de que los .env sean correctos). Ejecutar el comando ./tesina.sh seed para cargar la base de datos con los datos iniciales necesarios para el funcionamiento de las aplicaciones.","title":"Instalaci\u00f3n"},{"location":"instalacion/#despliegue","text":"Ejecutar el comando ./tesina.sh forever:startall para levantar todas las aplicaciones con forever. (Opcional) si no desea usar forever, puede ejecutar npm start en las carpetas crawl-extractors , search-engine , server y client . Para ejecutar el proceso de NLP ver en la siguiente secci\u00f3n.","title":"Despliegue"},{"location":"instalacion/#nlp","text":"El modulo de procesamiento de lenguaje general funciona de manera separada al resto de los componentes ya que no se integra al circuito de RabbitMQ. Para ejecutarlo debera hacer los siguientes pasos: Moverse a la carpeta nlp: cd nlp Activar el virtualenv: source venv/bin/activate Ejecutar el modulo principal con: python main.py El proceso comenzar\u00e1 a recuperar los art\u00edculos que NO fueron procesados desde la base de datos y los ira normalizando y persistiendo en la tabla de normalized_articles en la base de datos. Cada fila insertada contiene el texto resultante de la normalizaci\u00f3n as\u00ed como los pasos intermedios en cada columna. Al finalizar el proceso deber\u00eda tener la misma cantidad de art\u00edculos normalizados como art\u00edculos extra\u00eddos en su base de datos. AVISO : este proceso puede demorar bastante tiempo, se normalizan aproximadamente 10 art\u00edculos por minuto.","title":"NLP"},{"location":"instalacion/#usando-screen","text":"Probablemente, usted quiera dejar ejecutando el proceso en background, para ello utilizaremos screen. Instalar screen: apt-get install screen Moverse a la carpeta de NLP: cd nlp Escribir el comando screen: screen para entrar en la consola de screen. Activar el entorno virtual con: source venv/bin/activate Ejecutar el archivo main: python main.py Salir de screen presionando las teclas: ctrl + A + D al mismo tiempo, a partir de este punto puede cerrar su consola principal. Escriba el comando: screen -ls para comprobar que la consola de screen quedo en background Si desea retomar la consola, escriba: screen -r","title":"Usando screen"},{"location":"instalacion/#verificacion","text":"Para verificar si las aplicaciones est\u00e1n ejecutando, acceda desde el navegador a las siguientes rutas por defecto de las aplicaciones para comprobar si su api rest se encuentra activa. Server: http://localhost:8000/ Search Engine: http://localhost:8001/api/ Crawl Extractors: http://localhost:8002/api/ Client: http://localhost:8080 En cualquier caso, las aplicaciones mostraran un mensaje al comenzar su ejecuci\u00f3n ya sea indicando que workers est\u00e1n activos o en que puerto estan escuchando, esta es otra manera de verificar que las aplicaciones est\u00e1n ejecutando y lograron conectarse a la base de datos y RabbitMQ.","title":"Verificaci\u00f3n"},{"location":"instalacion/#configuraciones-adicionales","text":"Al seguir todos los pasos previos, el servidor crear\u00e1 en la base de datos una tabla de configuraciones relacionadas con el planificador y los request limite para los workers, as\u00ed como la cantidad de workers disponibles. Estas configuraciones se pueden modificar si lo desea accediendo con el script y comando ./tesina.sh config en donde puede configurar: [requestCount] - Cantidad de request realizados en el dia - un contador que muestra cuantos request se van realizando entre todos los workers, se resetea cuando todos los workers cumplen con su cuota diaria de requests, esta configuraci\u00f3n no es necesario que sea modificada. [requestLimit] - Cantidad de request limite por dia - utilice esta configuraci\u00f3n para setear cuantos request se pueden realizar por cada d\u00eda, tenga en cuenta es este valor se divide en partes iguales entre todos los workers para definir la cuota de request de cada uno. [workers] - Cantidad de workers disponibles - por defecto ya incluye todos los workers del sistema, en caso de incorporar nuevos workers deber\u00e1 actualizar este valor. Esta configuraci\u00f3n es importante ya que es utilizada para dividir la cantidad de request por cada worker por lo que su valor debe ser siempre el correcto. [scheduleOnStart] - \u00bfIniciar planificador al iniciar el servidor? - esta configuraci\u00f3n le permite habilitar o deshabilitar el planificador al iniciar el servidor. [scheduleEvery] - Periodicidad del planificador para el envi\u00f3 de ecuaciones - indica en forma de notaci\u00f3n crontab cada cuanto el planificador debe enviar las ecuaciones a RabbitMQ para ser ejecutadas, por defecto el planificador debe ejecutar una vez al d\u00eda cuando los workers hayan acabado su cuota diaria de requests. [nextDayCron] - Notacion Cron de ejecuci\u00f3n una vez por d\u00eda - notaci\u00f3n que indica en formato de crontab cuando es una ejecuci\u00f3n diaria, solo se guarda en caso de necesitar reemplazar el valor del [scheduleEvery] pero por el momento no se hace uso de esta configuraci\u00f3n. [wordcloudSchedulerOnStart] - \u00bfIniciar el planificador de construcci\u00f3n de nubes de palabras? - indica si se debe ejecutar el planificador que levanta los articulos normalizados y los procesa para armar la nube de palabras. [wordcloudSchedulerEvery] - Periodicidad del constructor de nubes de palabras - indica la periodicidad con la que se ejecutara el constructor de nube de palabras, por defecto se ejecuta cada 5 minutos procesando un m\u00e1ximo de 50 articulos.","title":"Configuraciones adicionales"}]}